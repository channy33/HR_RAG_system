import streamlit as st
from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain.docstore.document import Document
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS
from dotenv import load_dotenv
import os
import re
import tempfile

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
openai_api_key = os.getenv("OPENAI_API_KEY")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="RAGê¸°ë°˜ ì‚¬ë‚´ ì¸ì‚¬ê·œì • QA ì‹œìŠ¤í…œ", page_icon=":books:", layout="wide")

st.title(":books: ì¸ì‚¬ë‹´ë‹¹ìì™€ ì„ì§ì›ì„ ìœ„í•œ RAGê¸°ë°˜ ì‚¬ë‚´ ì¸ì‚¬ê·œì • QA ì‹œìŠ¤í…œ")
st.caption("ğŸ’¬ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì‚¬ë‚´ ê·œì •ì„ ì•Œë ¤ë“œë¦¬ëŠ” ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤ :) ì›í•˜ì‹œëŠ” ì‚¬ë‚´ ë¬¸ì„œë¥¼ ì™¼ìª½ ì‚¬ì´ë“œë°”ì— ì²¨ë¶€í•˜ì‹œê³ , ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”!")

# ìƒíƒœ ì´ˆê¸°í™”
if "conversation" not in st.session_state:
    st.session_state.conversation = None

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "processComplete" not in st.session_state:
    st.session_state.processComplete = False

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "messages" not in st.session_state:
    st.session_state.messages = []

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    uploaded_files = st.file_uploader("Upload your files", type=['pdf'], accept_multiple_files=True)
    openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
    process = st.button("Process")

    if st.session_state.processComplete:
        st.write("VectorDB was created")
        st.write(f"Document count: {st.session_state.doc_count}")

if process:
    if not openai_api_key:
        st.info("Please add your OpenAI API key to continue.")
        st.stop()

    if uploaded_files:
        all_pdf_docs = []
        for uploaded_file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_file_path = tmp_file.name

            def load_pdf(file_path):
                loader = PyPDFLoader(file_path)
                pdf_docs = loader.load_and_split()
                return pdf_docs

            pdf_docs = load_pdf(tmp_file_path)
            all_pdf_docs.extend(pdf_docs)

        def split_text(page_content):
            split_pattern = r'(?=ì œ\d+ì¡°(\ì˜\d+)?\([^)]+\)\s*[^ì œ]*)'
            split_text = re.split(split_pattern, page_content)
            cleaned_split_text = [part.strip() for part in split_text if part and len(part.strip()) > 20]
            return cleaned_split_text

        def extract_metadata(text, page, labor_id, source):
            match = re.search(r'ì œ(\d+ì¡°(\ì˜\d+)?)(\([^)]+\))\s*(.+)', text, re.DOTALL)
            if match:
                law_id = int(re.search(r'\d+', match.group(1)).group())
                law_num = f"ì œ{match.group(1).strip()}"
                title = match.group(3).strip('()')
                content = match.group(4).strip()
                return {
                    "labor_id": labor_id,
                    "law_id": law_id,
                    "law_num": law_num,
                    "title": title,
                    "page": page,
                    "source": source,
                    "contents": content
                }
            return None

        metadata = []
        labor_id_counter = 1
        end_of_data = False

        for page_num, page in enumerate(all_pdf_docs):
            if end_of_data:
                break
            page_content = page.page_content
            if '\në¶€     ì¹™ ' in page_content:
                page_content = page_content.split('\në¶€     ì¹™ ')[0]
                end_of_data = True
            cleaned_split_text = split_text(page_content)
            page_metadata = []
            for item in cleaned_split_text:
                meta = extract_metadata(item, page_num + 1, labor_id_counter, uploaded_file.name)
                if meta:
                    page_metadata.append(meta)
                    metadata.append(meta)
                    labor_id_counter += 1
            page.metadata = page_metadata

        content_list = []

        for doc in all_pdf_docs:
            for meta in doc.metadata:
                if isinstance(meta, dict):
                    content_list.append((meta['contents'], meta))

        final_docs = []

        for content, meta in content_list:
            new_doc = Document(page_content=content, metadata=meta)
            final_docs.append(new_doc)

        embeddings_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

        st.session_state.vectorstore = FAISS.from_documents(
            documents=final_docs,
            embedding=embeddings_model
        )

        st.session_state.conversation = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=openai_api_key),
            retriever=st.session_state.vectorstore.as_retriever(search_kwargs={"k": 2}),
            memory=ConversationBufferMemory(memory_key="chat_history", return_messages=True)
        )

        st.session_state.processComplete = True
        st.session_state.chat_history = []
        st.session_state.query_input = ""  # ì§ˆë¬¸ ì…ë ¥ í•„ë“œ ì´ˆê¸°í™”
        st.session_state.doc_count = len(all_pdf_docs)  # ë¬¸ì„œì˜ ì¥ìˆ˜ ì €ì¥

        # ì‚¬ì´ë“œë°”ì— VectorDB ìƒì„± ë©”ì‹œì§€ ë° ë¬¸ì„œ ì¥ìˆ˜ í‘œì‹œ
        with st.sidebar:
            st.write("VectorDB was created")
            st.write(f"Document count: {len(all_pdf_docs)}")

if st.session_state.processComplete:
    def handle_question():
        query = st.session_state.query_input
        response = st.session_state.conversation({"question": query, "chat_history": st.session_state.chat_history})
        answer = response.get("answer", "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.session_state.chat_history.append((query, answer))
        # ì…ë ¥ í•„ë“œ ì´ˆê¸°í™” ëŒ€ì‹  ì…ë ¥ ê°’ì„ ë¹„ì›Œì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
        st.session_state.query_input = ""

        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": query})
        st.session_state.messages.append({"role": "assistant", "content": answer})

    # ëŒ€í™” ê¸°ë¡ì„ í‘œì‹œ
    for message in st.session_state.messages:
        if message["role"] == "user":
            st.markdown(f"**ğŸ™‹ ì§ˆë¬¸:** {message['content']}")
        else:
            st.markdown(f"**ğŸ¤– ë‹µë³€:** {message['content']}")

            # ê·¼ê±° ë¬¸ì„œ ì¶”ê°€
            if message["role"] == "assistant":
                st.markdown("**ğŸ“„ ê·¼ê±° ë¬¸ì„œ:**")
                retrieved_docs = st.session_state.vectorstore.as_retriever().get_relevant_documents(message["content"])
                for doc in retrieved_docs[:2]:  # ê·¼ê±° ë¬¸ì„œ 2ê°œë§Œ í‘œì‹œ
                    meta = doc.metadata
                    st.markdown(f"**ì´ ê·œì¹™ì€ '{meta['title']}'ì— ëŒ€í•œ {meta['law_num']}ì¡°í•­ì…ë‹ˆë‹¤ (í˜ì´ì§€: {meta['page']}, ì¶œì²˜: {meta['source']})**")
                    st.markdown(doc.page_content[:150])

    # ìƒˆë¡œìš´ ì§ˆë¬¸ ì…ë ¥ í•„ë“œ
    query_input = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", key="query_input", value="")
    st.button("ì§ˆë¬¸ ì œì¶œ", on_click=handle_question)
